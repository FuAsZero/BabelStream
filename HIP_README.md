Here is guidance on how to write stream load/store performant HIP code.

1. Wider load/store can improve performance. Our ISA have global_load_dwordx4/global_store_dwordx4, which introduce less commands than global_load_dword/global_store_dword.

2. You can check the ISA code generated by compiler, ISA code can help you understand performance difference:
  a) export KMDUMPLLVM=1
     export KMDUMPISA=1
  b) compile your codes, you can find "dump-gfx906.isa" (if your are using MI50).

3. More threads in a WordGroup doesn't always introduce better performance. 
There are limited vGPR/sGPR/LDS resource in single SIMD/CU, more threads will reduce the vGPR/sGPR/LDS can be used by each thread. 
Original TBSIZE (threadBlock size) is 1024, here we suggest to use 128.

4. We have 60CU in MI50, each CU has 4 SIMD, totally 240 SIMD. 
Each WorkGroup will be dispatched on one SIMD, so when you change DOT_NUM_BLOCKS (number of WorkGroups for DOT kernel) from 256 to 240, you can see performance improvement.
And please remember to change ARRAY_SIZE compliant with DOT_NUM_BLOCKS.

5. Compiler can do some optimization works, but compiler may not be as smart as you expected.
Take add_kernel as an example, to generate dwordx4 load/store ISA, you may write 4 continous float load/store as:
  c[i]   = a[i]   + b[i];
  c[i+1] = a[i+1] + b[i+1];
  c[i+2] = a[i+2] + b[i+2];
  c[i+3] = a[i+3] + b[i+3];

If you are not satisfied with performance, you can change your code as:
  //explicit address offset can avoid redundant address calculation ISA.
  a += i;
  b += i;
  c += i;

  //compiler can generate dwordx4 load to vGPR for the 4 consecutive dword load, as this is more straightforward for compiler to optimize
  float v0 = a[0];
  float v1 = a[1];
  float v2 = a[2];
  float v3 = a[3];

  float v4 = b[0];
  float v5 = b[1];
  float v6 = b[2];
  float v7 = b[3];

  c[0] = v0 + v4;
  c[1] = v1 + v5;
  c[2] = v2 + v6;
  c[3] = v3 + v7;

6. If you are dealing with double datatype, you need 2 double to assemble a dwordx4 load/store. 
We have dwordx2 support, so you can find double type performs better than float type in original single data operator.

7. Summary:
  a) let compiler generate as less address calculation ISA as possible;
  b) let compiler easily know it can assemble consecutive load/store into dwordx4 operator;
  c) HW support at most 1024 threads per WorkGroup, but more threads indicate less HW resource, more conflict;
  d) don't expect compiler can do all optimizations you want, try to 'tell' compiler your idea.